{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "popular\n",
    "\n",
    "### Setup\n",
    "neurons w/ weights $w$ (+ biases $b$) and nonlinearity/activation $\\phi$\n",
    "\n",
    "$\\phi(\\sum_i x_i w_i + b_i)$\n",
    "\n",
    "In layers w/ weights $W  \\in \\mathbb{R}^{n_l \\times n_{l+1}}$ and biases $b_l \\in \\mathbb{R}^{n_k}$ w/ $n_l$ neurons in layer $l$:\n",
    "$$\\phi(W_{l}x_l + b_l)$$ \n",
    "(abuse of notation w/ $\\phi$)\n",
    "\n",
    "(if input points are $x \\in \\mathbb{R}^d$, then $n_l = d$)\n",
    "\n",
    "Do this for all layers to get some output values in your final layer (*forward pass*)\n",
    "\n",
    "set initial weights $W_{l}$ randomly\n",
    "\n",
    "*Tons* of different shapes/types of NNs\n",
    "\n",
    "split data into  train and test (80/20ish is good)\n",
    "\n",
    "### Backpropagation\n",
    "Loss $L(y)$ is a function of the output $y$ and the target $t$, e.g.:\n",
    "$$L(y) = (t-j)^2$$\n",
    "\n",
    "Calculate derivative wrt each weight $D_n = \\frac{\\partial L(y)}{\\partial w_n}$ and use gradient descent to update weights:\n",
    "$$w_n \\leftarrow w_n - \\eta D_n$$\n",
    "for learning rate $\\eta > 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(as_frame = True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.repeat(20).shuffle(1000).batch(32)\n",
    "test = test.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 1s 3ms/step - loss: 1.2829 - accuracy: 0.3292 - val_loss: 1.1317 - val_accuracy: 0.3667\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.1326 - accuracy: 0.2912 - val_loss: 1.0770 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0597 - accuracy: 0.3508 - val_loss: 1.0327 - val_accuracy: 0.4333\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0013 - accuracy: 0.5467 - val_loss: 0.9714 - val_accuracy: 0.6333\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9382 - accuracy: 0.6058 - val_loss: 0.9125 - val_accuracy: 0.6333\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8886 - accuracy: 0.6029 - val_loss: 0.8520 - val_accuracy: 0.6333\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.8368 - accuracy: 0.6071 - val_loss: 0.7963 - val_accuracy: 0.6333\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8034 - accuracy: 0.6075 - val_loss: 0.7507 - val_accuracy: 0.6333\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7824 - accuracy: 0.6029 - val_loss: 0.7095 - val_accuracy: 0.6333\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.7626 - accuracy: 0.6017 - val_loss: 0.6747 - val_accuracy: 0.6333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00181a6740>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),   # hidden layer\n",
    "    # tf.keras.layers.Dense(10, activation=tf.nn.relu),   # hidden layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation=tf.nn.softmax)  # output layer\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train,\n",
    "    validation_data=test,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prediction is 'setosa' (100.0%), expected 'setosa'\n",
      "✓ Prediction is 'versicolor' (99.8%), expected 'versicolor'\n",
      "✓ Prediction is 'virginica' (91.9%), expected 'virginica'\n"
     ]
    }
   ],
   "source": [
    "for pred_dict, expected in zip(predictions, [\"setosa\", \"versicolor\", \"virginica\"]):\n",
    "    predicted_index = pred_dict.argmax()\n",
    "    predicted = load_iris().target_names[predicted_index]\n",
    "    probability = pred_dict.max()\n",
    "    tick_cross = \"✓\" if predicted == expected else \"✗\"\n",
    "    print(f\"{tick_cross} Prediction is '{predicted}' ({100 * probability:.1f}%), expected '{expected}'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "### Image Kernel Convolutions\n",
    "images are matrices of pixel values\n",
    "use kernel to convolve over image to get new image (using padding at edges maybe so output image is same size as input image---e.g. zero padding (add a border of zeros) or mirror padding (add a border of identical pixels to the edge pixels))\n",
    "\n",
    "e.g. for kernel $w$ and image w/ pixel coords $f(x,y)$ we get pixel value $g(x,y)$ where:\n",
    "$$g(x,y) = w \\ast f(x,y) = \\sum_{dx=-a}^a \\sum_{dy=-b}^b w(dx,dy) f(x-dx, y-dy)$$\n",
    "\n",
    "e.g. for a 3x3 kernel:\n",
    "$$ w = \\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1 \\end{bmatrix}$$\n",
    "\n",
    "(^^directional edge detection kernel (I think?))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs\n",
    "\n",
    "#### Convolutional Layers\n",
    "We'll make an NN learn the convolution kernels for us! (i.e. learn the weights $w_{x,y}(dx,dy)$---i.e. the weights of the kernel depend on the pixels being convolved over.)\n",
    "And we can stack these layers to get more complex kernels.\n",
    "\n",
    "#### Pooling Layers\n",
    "We can also use pooling layers to reduce the size of the image (e.g. max pooling). These just take a window of pixels and output the max value (or average value or something), meaning we can reduce the size of the image without losing too much information (downsampling).\n",
    "\n",
    "#### Fully Connected Layers (FC/Dense Layers)\n",
    "Fully connected layers are just like the ones we've seen before (i.e. in non-convolution-land), but we flatten the image first (i.e. we take the image and turn it into a vector of pixel values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    ),\n",
    "    tf.keras.layers.MaxPool2D((2, 2), (2, 2), padding=\"same\"),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    ),\n",
    "    tf.keras.layers.MaxPool2D((2, 2), (2, 2), padding=\"same\"),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    ),\n",
    "    tf.keras.layers.MaxPool2D((2, 2), (2, 2), padding=\"same\"),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    ),\n",
    "    tf.keras.layers.MaxPool2D((2, 2), (2, 2), padding=\"same\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds_train, ds_test = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(normalize_img)\n",
    "\n",
    "ds_train = ds_train.shuffle(1000)\n",
    "ds_train = ds_train.batch(128)\n",
    "\n",
    "ds_test = ds_test.map(normalize_img)\n",
    "ds_test = ds_test.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0366 - val_accuracy: 0.9935\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0478 - val_accuracy: 0.9932\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 20s 42ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0496 - val_accuracy: 0.9928\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0611 - val_accuracy: 0.9921\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0441 - val_accuracy: 0.9925\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0510 - val_accuracy: 0.9925\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0825 - val_accuracy: 0.9900\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0604 - val_accuracy: 0.9918\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0536 - val_accuracy: 0.9928\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0600 - val_accuracy: 0.9919\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0515 - val_accuracy: 0.9928\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0688 - val_accuracy: 0.9926\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0709 - val_accuracy: 0.9923\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0554 - val_accuracy: 0.9933\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0607 - val_accuracy: 0.9932\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0698 - val_accuracy: 0.9920\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0553 - val_accuracy: 0.9943\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0640 - val_accuracy: 0.9934\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0624 - val_accuracy: 0.9926\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0649 - val_accuracy: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff9b45e920>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_test,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "for i in list(range(1,10)) + [\"dog\"]:\n",
    "    urlretrieve(f\"https://github.com/milliams/intro_deep_learning/raw/master/{i}.png\", f\"{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 28, 28, 1)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "\n",
    "images = []\n",
    "for i in list(range(1,10)) + [\"dog\"]:\n",
    "    images.append(np.array(imread(f\"{i}.png\")/255.0, dtype=\"float32\"))\n",
    "images = np.array(images)[:,:,:,np.newaxis]\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "probabilities = model.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 at 51.3%. CNN thinks it's a 1 (51.3%)\n",
      "2 at 84.8%. CNN thinks it's a 2 (84.8%)\n",
      "3 at 91.2%. CNN thinks it's a 3 (91.2%)\n",
      "4 at  0.1%. CNN thinks it's a 5 (41.7%)\n",
      "5 at 100.0%. CNN thinks it's a 5 (100.0%)\n",
      "6 at  0.0%. CNN thinks it's a 3 (99.9%)\n",
      "7 at 99.7%. CNN thinks it's a 7 (99.7%)\n",
      "8 at  4.7%. CNN thinks it's a 1 (21.8%)\n",
      "9 at 15.2%. CNN thinks it's a 8 (64.3%)\n",
      "dog. CNN thinks it's a 8 (17.0%)\n"
     ]
    }
   ],
   "source": [
    "truths = list(range(1, 10)) + [\"dog\"]\n",
    "\n",
    "table = []\n",
    "for truth, probs in zip(truths, probabilities):\n",
    "    prediction = probs.argmax()\n",
    "    if truth == 'dog':\n",
    "        print(f\"{truth}. CNN thinks it's a {prediction} ({probs[prediction]*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{truth} at {probs[truth]*100:4.1f}%. CNN thinks it's a {prediction} ({probs[prediction]*100:4.1f}%)\")\n",
    "    table.append((truth, probs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "add inveted images to training data to make the NN more robust to different images\n",
    "(could also do rotated images, &c.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 41s 44ms/step - loss: 0.0636 - accuracy: 0.9816 - val_loss: 0.0645 - val_accuracy: 0.9863\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 41s 43ms/step - loss: 0.0300 - accuracy: 0.9918 - val_loss: 0.0691 - val_accuracy: 0.9855\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.0209 - accuracy: 0.9942 - val_loss: 0.0529 - val_accuracy: 0.9911\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.0178 - accuracy: 0.9953 - val_loss: 0.0709 - val_accuracy: 0.9897\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 42s 44ms/step - loss: 0.0152 - accuracy: 0.9960 - val_loss: 0.0499 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff982947c0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train, ds_test = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,\n",
    ")\n",
    "\n",
    "def invert_img(image, label):\n",
    "    return 1.-image, label\n",
    "\n",
    "ds_train = ds_train.map(normalize_img)\n",
    "ds_train = ds_train.concatenate(ds_train.map(invert_img))  # new line\n",
    "ds_train = ds_train.shuffle(1000)\n",
    "ds_train = ds_train.batch(128)\n",
    "\n",
    "ds_test = ds_test.map(normalize_img)\n",
    "ds_test = ds_test.concatenate(ds_test.map(invert_img))  # new line\n",
    "ds_test = ds_test.batch(128)\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_test,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "probabilities = model.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 at 67.6%. CNN thinks it's a 1 (67.6%)\n",
      "2 at 100.0%. CNN thinks it's a 2 (100.0%)\n",
      "3 at 99.9%. CNN thinks it's a 3 (99.9%)\n",
      "4 at 99.9%. CNN thinks it's a 4 (99.9%)\n",
      "5 at 100.0%. CNN thinks it's a 5 (100.0%)\n",
      "6 at 100.0%. CNN thinks it's a 6 (100.0%)\n",
      "7 at 100.0%. CNN thinks it's a 7 (100.0%)\n",
      "8 at 100.0%. CNN thinks it's a 8 (100.0%)\n",
      "9 at  1.8%. CNN thinks it's a 8 (84.6%)\n",
      "dog. CNN thinks it's a 8 (42.9%)\n"
     ]
    }
   ],
   "source": [
    "truths = list(range(1, 10)) + [\"dog\"]\n",
    "\n",
    "table = []\n",
    "for truth, probs in zip(truths, probabilities):\n",
    "    prediction = probs.argmax()\n",
    "    if truth == 'dog':\n",
    "        print(f\"{truth}. CNN thinks it's a {prediction} ({probs[prediction]*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{truth} at {probs[truth]*100:4.1f}%. CNN thinks it's a {prediction} ({probs[prediction]*100:4.1f}%)\")\n",
    "    table.append((truth, probs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

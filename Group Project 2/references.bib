
@article{rabiner_tutorial_1989,
	title = {A tutorial on hidden {Markov} models and selected applications in speech recognition},
	volume = {77},
	issn = {1558-2256},
	doi = {10.1109/5.18626},
	abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{\textless}{\textgreater}},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Rabiner, L.R.},
	month = feb,
	year = {1989},
	keywords = {Hidden Markov models, Speech recognition, Tutorial},
	pages = {257--286},
}

@article{metropolis_equation_1953,
	title = {Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
	volume = {21},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.1699114},
	doi = {10.1063/1.1699114},
	language = {en},
	number = {6},
	urldate = {2023-02-03},
	journal = {The Journal of Chemical Physics},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
	month = jun,
	year = {1953},
	pages = {1087--1092},
	file = {Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf:/home/dg22309/Zotero/storage/UITYPMXQ/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf:application/pdf},
}

@article{hastings_monte_1970,
	title = {Monte {Carlo} {Sampling} {Methods} {Using} {Markov} {Chains} and {Their} {Applications}},
	abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
	language = {en},
	author = {Hastings, W K},
	year = {1970},
	file = {Hastings - 2023 - Monte Carlo Sampling Methods Using Markov Chains a.pdf:/home/dg22309/Zotero/storage/F2DEIDLR/Hastings - 2023 - Monte Carlo Sampling Methods Using Markov Chains a.pdf:application/pdf},
}

@book{doucet_sequential_2001,
	address = {New York, NY},
	title = {Sequential {Monte} {Carlo} {Methods} in {Practice}},
	isbn = {978-1-4419-2887-0 978-1-4757-3437-9},
	url = {http://link.springer.com/10.1007/978-1-4757-3437-9},
	urldate = {2023-05-30},
	publisher = {Springer},
	editor = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
	year = {2001},
	doi = {10.1007/978-1-4757-3437-9},
	keywords = {artificial intelligence, bayesian statistics, calculus, data analysis, dynamic models, econometrics, Likelihood, machine learning, modeling, Monte Carlo Methods, neural networks, Resampling, Statistical Models, statistics, time series analysis},
	file = {doucet_defreitas_gordon_smcbookintro.pdf:/home/dg22309/Zotero/storage/SCITUYE9/doucet_defreitas_gordon_smcbookintro.pdf:application/pdf;Full Text:/home/dg22309/Zotero/storage/NZFYMJII/Doucet et al. - 2001 - Sequential Monte Carlo Methods in Practice.pdf:application/pdf},
}

@article{zhang_sequential_2023,
	title = {Sequential {Gaussian} {Processes} for {Online} {Learning} of {Nonstationary} {Functions}},
	volume = {71},
	issn = {1053-587X, 1941-0476},
	url = {https://ieeexplore.ieee.org/document/10103648/},
	doi = {10.1109/TSP.2023.3267992},
	abstract = {We propose a sequential Monte Carlo algorithm to ﬁt inﬁnite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods for online GP estimation in the presence of non-stationarity in time-series data. To demonstrate the utility of our proposed online Gaussian process mixture-of-experts approach in applied settings, we show that we can successfully implement an optimization algorithm using online Gaussian process bandits.},
	language = {en},
	urldate = {2023-05-30},
	journal = {IEEE Transactions on Signal Processing},
	author = {Zhang, Michael Minyi and Dumitrascu, Bianca and Williamson, Sinead A. and Engelhardt, Barbara E.},
	year = {2023},
	pages = {1539--1550},
	file = {Zhang et al. - 2023 - Sequential Gaussian Processes for Online Learning .pdf:/home/dg22309/Zotero/storage/DWU5MQR6/Zhang et al. - 2023 - Sequential Gaussian Processes for Online Learning .pdf:application/pdf},
}

@article{abdessalem_automatic_2017,
	title = {Automatic {Kernel} {Selection} for {Gaussian} {Processes} {Regression} with {Approximate} {Bayesian} {Computation} and {Sequential} {Monte} {Carlo}},
	volume = {3},
	issn = {2297-3362},
	url = {https://www.frontiersin.org/articles/10.3389/fbuil.2017.00052},
	abstract = {The current work introduces a novel combination of two Bayesian tools, Gaussian Processes (GPs), and the use of the Approximate Bayesian Computation (ABC) algorithm for kernel selection and parameter estimation for machine learning applications. The combined methodology that this research article proposes and investigates offers the possibility to use different metrics and summary statistics of the kernels used for Bayesian regression. The presented work moves a step toward online, robust, consistent, and automated mechanism to formulate optimal kernels (or even mean functions) and their hyperparameters simultaneously offering confidence evaluation when these tools are used for mathematical or engineering problems such as structural health monitoring (SHM) and system identification (SI).},
	urldate = {2023-05-30},
	journal = {Frontiers in Built Environment},
	author = {Abdessalem, Anis Ben and Dervilis, Nikolaos and Wagg, David J. and Worden, Keith},
	year = {2017},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/GZQ3ZLTT/Abdessalem et al. - 2017 - Automatic Kernel Selection for Gaussian Processes .pdf:application/pdf},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9615956},
	urldate = {2023-05-30},
	file = {IEEE Xplore Full-Text PDF\::/home/dg22309/Zotero/storage/D7Q6JCK6/stamp.html:text/html},
}

@book{chopin_introduction_2020,
	address = {Cham},
	series = {Springer {Series} in {Statistics}},
	title = {An {Introduction} to {Sequential} {Monte} {Carlo}},
	isbn = {978-3-030-47844-5 978-3-030-47845-2},
	url = {https://link.springer.com/10.1007/978-3-030-47845-2},
	language = {en},
	urldate = {2023-05-30},
	publisher = {Springer International Publishing},
	author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
	year = {2020},
	doi = {10.1007/978-3-030-47845-2},
	keywords = {Hidden Markov models, Bayesian inference, data-driven science, modeling and theory building, Feynman-Kac models, Markov chain Monte Carlo, Particle filter, Sequential learning, Sequential Monte Carlo, State-space models},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/UZK5ZMZ8/Chopin and Papaspiliopoulos - 2020 - An Introduction to Sequential Monte Carlo.pdf:application/pdf},
}

@article{buchholz_adaptive_2021,
	title = {Adaptive {Tuning} of {Hamiltonian} {Monte} {Carlo} {Within} {Sequential} {Monte} {Carlo}},
	volume = {16},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-3/Adaptive-Tuning-of-Hamiltonian-Monte-Carlo-Within-Sequential-Monte-Carlo/10.1214/20-BA1222.full},
	doi = {10.1214/20-BA1222},
	abstract = {Sequential Monte Carlo (SMC) samplers are an alternative to MCMC for Bayesian computation. However, their performance depends strongly on the Markov kernels used to rejuvenate particles. We discuss how to calibrate automatically (using the current particles) Hamiltonian Monte Carlo kernels within SMC. To do so, we build upon the adaptive SMC approach of Fearnhead and Taylor (2013), and we also suggest alternative methods. We illustrate the advantages of using HMC kernels within an SMC sampler via an extensive numerical study.},
	number = {3},
	urldate = {2023-05-30},
	journal = {Bayesian Analysis},
	author = {Buchholz, Alexander and Chopin, Nicolas and Jacob, Pierre E.},
	month = sep,
	year = {2021},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {62F15, 65C05, Hamiltonian Monte Carlo, sequential Monte Carlo},
	pages = {745--771},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/DPGJIIH3/Buchholz et al. - 2021 - Adaptive Tuning of Hamiltonian Monte Carlo Within .pdf:application/pdf},
}

@article{nguyen_efficient_2016,
	title = {Efficient {Sequential} {Monte}-{Carlo} {Samplers} for {Bayesian} {Inference}},
	volume = {64},
	issn = {1941-0476},
	doi = {10.1109/TSP.2015.2504342},
	abstract = {In many problems, complex non-Gaussian and/or nonlinear models are required to accurately describe a physical system of interest. In such cases, Monte-Carlo algorithms are remarkably flexible and extremely powerful approaches to solve such inference problems. However, in the presence of a high-dimensional and/or multimodal posterior distribution, it is widely documented that standard Monte-Carlo techniques could lead to poor performance. In this paper, the study is focused on a Sequential Monte-Carlo (SMC) sampler framework, a more robust and efficient Monte-Carlo algorithm. Although this approach presents many advantages over traditional Monte-Carlo methods, the potential of this emergent technique is, however, largely underexploited in signal processing. In this paper, we aim at proposing some novel strategies to improve the efficiency and facilitate practical implementation of the SMC sampler. First, we propose an automatic and adaptive strategy that selects the sequence of distributions within the SMC sampler that minimizes the asymptotic variance of the estimator of the posterior normalization constant. The second original contribution we present improves the global efficiency of the SMC sampler by introducing a novel correction mechanism that allows the use of the particles generated through all of the iterations of the algorithm (instead of only particles from the last iteration). This is a significant contribution as it removes the need to discard a large portion of the samples obtained, as is standard in standard SMC methods. This will improve estimation performance in practical settings where the computational budget is important to consider.},
	number = {5},
	journal = {IEEE Transactions on Signal Processing},
	author = {Nguyen, Thi Le Thu and Septier, François and Peters, Gareth W. and Delignon, Yves},
	month = mar,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Signal Processing},
	keywords = {Bayesian inference, Bayes methods, complex models, Kernel, Markov processes, Monte Carlo methods, sequential Monte Carlo sampler, Signal processing, Signal processing algorithms, Standards},
	pages = {1305--1319},
	file = {IEEE Xplore Abstract Record:/home/dg22309/Zotero/storage/UUPMGNND/stamp.html:text/html;IEEE Xplore Full Text PDF:/home/dg22309/Zotero/storage/YGUPXECQ/Nguyen et al. - 2016 - Efficient Sequential Monte-Carlo Samplers for Baye.pdf:application/pdf},
}

@article{cappe_overview_2007,
	title = {An {Overview} of {Existing} {Methods} and {Recent} {Advances} in {Sequential} {Monte} {Carlo}},
	volume = {95},
	doi = {10.1109/JPROC.2007.893250},
	abstract = {It is now over a decade since the pioneering contribution of Gordon (1993), which is commonly regarded as the first instance of modern sequential Monte Carlo (SMC) approaches. Initially focussed on applications to tracking and vision, these techniques are now very widespread and have had a significant impact in virtually all areas of signal and image processing concerned with Bayesian dynamical models. This paper is intended to serve both as an introduction to SMC algorithms for nonspecialists and as a reference to recent contributions in domains where the techniques are still under significant development, including smoothing, estimation of fixed parameters and use of SMC methods beyond the standard filtering contexts.},
	journal = {Proceedings of the IEEE},
	author = {Cappe, Olivier and Godsill, S.J. and Moulines, Eric},
	month = jun,
	year = {2007},
	pages = {899--924},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/2G7J9HSH/Cappe et al. - 2007 - An Overview of Existing Methods and Recent Advance.pdf:application/pdf},
}

@article{dobigeon_nonlinear_2014,
	title = {Nonlinear unmixing of hyperspectral images: models and algorithms},
	volume = {31},
	issn = {1053-5888},
	shorttitle = {Nonlinear unmixing of hyperspectral images},
	url = {http://arxiv.org/abs/1304.1875},
	doi = {10.1109/MSP.2013.2279274},
	abstract = {When considering the problem of unmixing hyperspectral images, most of the literature in the geoscience and image processing areas relies on the widely used linear mixing model (LMM). However, the LMM may be not valid and other nonlinear models need to be considered, for instance, when there are multi-scattering effects or intimate interactions. Consequently, over the last few years, several signiﬁcant contributions have been proposed to overcome the limitations inherent in the LMM. In this paper, we present an overview of recent advances in nonlinear unmixing modeling.},
	language = {en},
	number = {1},
	urldate = {2023-05-30},
	journal = {IEEE Signal Processing Magazine},
	author = {Dobigeon, Nicolas and Tourneret, Jean-Yves and Richard, Cédric and Bermudez, José C. M. and McLaughlin, Stephen and Hero, Alfred O.},
	month = jan,
	year = {2014},
	note = {arXiv:1304.1875 [physics, stat]},
	keywords = {Statistics - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Applications, Statistics - Methodology},
	pages = {82--94},
	file = {Dobigeon et al. - 2014 - Nonlinear unmixing of hyperspectral images models.pdf:/home/dg22309/Zotero/storage/C4C2ZWJI/Dobigeon et al. - 2014 - Nonlinear unmixing of hyperspectral images models.pdf:application/pdf},
}

@article{andrieu_particle_2010,
	title = {Particle {Markov} {Chain} {Monte} {Carlo} {Methods}},
	volume = {72},
	issn = {1369-7412, 1467-9868},
	url = {https://academic.oup.com/jrsssb/article/72/3/269/7076437},
	doi = {10.1111/j.1467-9868.2009.00736.x},
	abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efﬁcient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a Lévy-driven stochastic volatility model.},
	language = {en},
	number = {3},
	urldate = {2023-05-30},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
	month = jun,
	year = {2010},
	pages = {269--342},
	file = {Andrieu et al. - 2010 - Particle Markov Chain Monte Carlo Methods.pdf:/home/dg22309/Zotero/storage/EDK2AG5B/Andrieu et al. - 2010 - Particle Markov Chain Monte Carlo Methods.pdf:application/pdf},
}

@article{abbasi_robust_2020,
	title = {A {Robust} and {Accurate} {Particle} {Filter}-{Based} {Pupil} {Detection} {Method} for {Big} {Datasets} of {Eye} {Video}},
	volume = {18},
	issn = {1572-9184},
	url = {https://doi.org/10.1007/s10723-019-09502-1},
	doi = {10.1007/s10723-019-09502-1},
	abstract = {Accurate detection of pupil position in successive frames of eye videos is finding applications in many areas including assistive systems and E-learning. Processing the big datasets of eye videos in such systems requires robust and fast eye-tracking algorithms that can predict the position of eye pupil in consecutive video frames. As a major technique, particle filters provide adequate speed but have a low detection rate. To solve this problem, the present paper suggests the use of genetic algorithms in the sampling step of the particle filter technique. As a result, in each frame, the variety of particles required for predicting the pupil position in the next video frame is maintained and their uniformity is reduced. Finally, the speed and detection rate of the proposed method, as well as the basic particle filter method in predicting the pupil position in video frames are calculated and compared for various populations. The experimental results indicate that, in comparison with the basic particle filter algorithm, the proposed algorithm detects the pupil more accurately and in a shorter time. Also, by achieving an average detection rate of 79.89\% in estimation of the pupil center with an error of five pixels on a variety of eye videos with different situations of occlusion and illumination, not only the robustness of the proposed method is assessed but also its superiority to the state-of-the-art methods is evinced.},
	language = {en},
	number = {2},
	urldate = {2023-05-30},
	journal = {Journal of Grid Computing},
	author = {Abbasi, Mahdi and Khosravi, Mohammad R.},
	month = jun,
	year = {2020},
	keywords = {Particle filter, Big dataset, Detection rate, Eye tracking, Genetic algorithm, Pupil},
	pages = {305--325},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/SEVUM2EL/Abbasi and Khosravi - 2020 - A Robust and Accurate Particle Filter-Based Pupil .pdf:application/pdf},
}

@book{halimeh_neural_2019,
	title = {Neural {Networks} {Sequential} {Training} {Using} {Variational} {Gaussian} {Particle} {Filter}},
	author = {Halimeh, Modar and Brendel, Andreas and Kellermann, Walter},
	month = may,
	year = {2019},
	doi = {10.1109/ICASSP.2019.8683886},
	note = {Pages: 3006},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/ZSEKMTYZ/Halimeh et al. - 2019 - Neural Networks Sequential Training Using Variatio.pdf:application/pdf},
}

@inproceedings{gordon_novel_1993,
	title = {Novel approach to nonlinear/non-{Gaussian} {Bayesian} state estimation},
	volume = {140},
	url = {https://digital-library.theiet.org/content/journals/10.1049/ip-f-2.1993.0015},
	doi = {10.1049/ip-f-2.1993.0015},
	abstract = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linear- ity or Gaussian noise: it may be applied to any state transition or measurement model. A simula- tion example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {{IEE} {Proceedings} {F} {Radar} and {Signal} {Processing}},
	author = {Gordon, N.J. and Salmond, D.J. and Smith, A.F.M.},
	year = {1993},
	note = {ISSN: 0956375X
Issue: 2
Journal Abbreviation: IEE Proc. F Radar Signal Process. UK},
	pages = {107},
	annote = {[TLDR] An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters, represented as a set of random samples, which are updated and propagated by the algorithm.},
	file = {Gordon et al. - 1993 - Novel approach to nonlinearnon-Gaussian Bayesian .pdf:/home/dg22309/Zotero/storage/Y7NTZZWS/Gordon et al. - 1993 - Novel approach to nonlinearnon-Gaussian Bayesian .pdf:application/pdf},
}

@article{pitt_filtering_1997,
	title = {Filtering via {Simulation}: {Auxiliary} {Particle} {Filters}},
	volume = {94},
	shorttitle = {Filtering via {Simulation}},
	doi = {10.2307/2670179},
	abstract = {This paper analyses the recently suggested particle approach to filtering time series. We suggest that the algorithm is not robust to outliers for two reasons: the design of the simulators and the use of the discrete support to represent the sequentially updating prior distribution. Both problems are tackled in this paper. We believe we have largely solved the first problem and have reduced the order of magnitude of the second. In addition we introduce the idea of stratification into the particle filter which allows us to perform on-line Bayesian calculations about the parameters which index the models and maximum likelihood estimation. The new methods are illustrated by using a stochastic volatility model and a time series model of angles. Some key words: Filtering, Markov chain Monte Carlo, Particle filter, Simulation, SIR, State space. 1 1},
	journal = {Journal of the American Statistical Association},
	author = {Pitt, Michael and Shephard, Neil},
	month = nov,
	year = {1997},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/289RRXVH/Pitt and Shephard - 1997 - Filtering via Simulation Auxiliary Particle Filte.pdf:application/pdf},
}

@misc{daviet_inference_2018,
	title = {Inference with {Hamiltonian} {Sequential} {Monte} {Carlo} {Simulators}},
	url = {http://arxiv.org/abs/1812.07978},
	abstract = {The paper proposes a new Monte-Carlo simulator combining the advantages of Sequential Monte Carlo simulators and Hamiltonian Monte Carlo simulators. The result is a method that is robust to multimodality and complex shapes to use for inference in presence of diﬃcult likelihoods or target functions. Several examples are provided.},
	language = {en},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Daviet, Remi},
	month = dec,
	year = {2018},
	note = {arXiv:1812.07978 [stat]},
	keywords = {Statistics - Computation},
	file = {Daviet - 2018 - Inference with Hamiltonian Sequential Monte Carlo .pdf:/home/dg22309/Zotero/storage/J5C9IIF7/Daviet - 2018 - Inference with Hamiltonian Sequential Monte Carlo .pdf:application/pdf},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2023-05-31},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv:1601.00670 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
	pages = {859--877},
	file = {arXiv Fulltext PDF:/home/dg22309/Zotero/storage/4QSIEMWJ/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:/home/dg22309/Zotero/storage/6SU52IKS/1601.html:text/html},
}

@inproceedings{kurihara_bayesian_2001,
	title = {Bayesian on-line learning: a sequential {Monte} {Carlo} with importance resampling},
	shorttitle = {Bayesian on-line learning},
	doi = {10.1109/NNSP.2001.943121},
	abstract = {A Bayesian online learning scheme with sequential Monte Carlo incorporating importance resampling is proposed. The proposed scheme adjusts not only parameters for data fitting but also adjusts hyperparameters online so that the scheme attempts to avoid overfitting in an adaptive manner. One of the advantages of the scheme is the fact that it can adapt to environmental changes, i.e., it can perform learning, even when the underlying input-output relationship varies over time. The scheme is tested against simple examples and is shown to be functional.},
	booktitle = {Neural {Networks} for {Signal} {Processing} {XI}: {Proceedings} of the 2001 {IEEE} {Signal} {Processing} {Society} {Workshop} ({IEEE} {Cat}. {No}.{01TH8584})},
	author = {Kurihara, T. and Nakada, Y. and Yosui, K. and Matsumoto, T.},
	month = sep,
	year = {2001},
	note = {ISSN: 1089-3555},
	keywords = {Bayesian methods, Integral equations, Monte Carlo methods, Nonlinear equations, Sequential analysis, State estimation, Testing, Training data, Uncertainty},
	pages = {163--172},
	file = {IEEE Xplore Abstract Record:/home/dg22309/Zotero/storage/4JDS5CUQ/stamp.html:text/html;IEEE Xplore Full Text PDF:/home/dg22309/Zotero/storage/UW53NKPS/Kurihara et al. - 2001 - Bayesian on-line learning a sequential Monte Carl.pdf:application/pdf},
}

@article{kalman_new_1960,
	title = {A {New} {Approach} to {Linear} {Filtering} and {Prediction} {Problems}},
	volume = {82},
	issn = {0021-9223},
	url = {https://doi.org/10.1115/1.3662552},
	doi = {10.1115/1.3662552},
	abstract = {The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
	number = {1},
	urldate = {2023-05-31},
	journal = {Journal of Basic Engineering},
	author = {Kalman, R. E.},
	month = mar,
	year = {1960},
	pages = {35--45},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/2SUX4E6N/Kalman - 1960 - A New Approach to Linear Filtering and Prediction .pdf:application/pdf;Snapshot:/home/dg22309/Zotero/storage/3QN6LF2U/A-New-Approach-to-Linear-Filtering-and-Prediction.html:text/html},
}

@article{kitagawa_non-gaussian_1987,
	title = {Non-{Gaussian} {State}-{Space} {Modeling} of {Nonstationary} {Time} {Series}},
	volume = {82},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2289375},
	doi = {10.2307/2289375},
	abstract = {A non-Gaussian state-space approach to the modeling of nonstationary time series is shown. The model is expressed in state-space form, where the system noise and the observational noise are not necessarily Gaussian. Recursive formulas of prediction, filtering, and smoothing for the state estimation and identification of the non-Gaussian state-space model are given. Also given is a numerical method based on piecewise linear approximation to the density functions for realizing these formulas. Significant merits of non-Gaussian modeling and the wide range of applicability of the method are illustrated by some numerical examples. A typical application of this non-Gaussian modeling is the smoothing of a time series that has mean value function with both abrupt and gradual changes. Simple Gaussian state-space modeling is not adequate for this situation. Here the model with small system noise variance cannot detect jump, whereas the one with large system noise variance yields unfavorable wiggle. To work out this problem within the ordinary linear Gaussian model framework, sophisticated treatment of outliers is required. But by the use of an appropriate non-Gaussian model for system noise, it is possible to reproduce both abrupt and gradual change of the mean without any special treatment. Nonstandard observations such as the ones distributed as non-Gaussian distribution can be easily treated by the direct modeling of an observational scheme. Smoothing of a transformed series such as a log periodogram can be treated by this method. Outliers in the observations can be treated as well by using heavy-tailed distribution for observational noise density. The algorithms herein can be easily extended to a wider class of models. As an example, the smoothing of nonhomogeneous binomial mean function is shown, where the observation is distributed according to a discrete random variable. Extension to a nonlinear system is also straightforward.},
	number = {400},
	urldate = {2023-05-31},
	journal = {Journal of the American Statistical Association},
	author = {Kitagawa, Genshiro},
	year = {1987},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1032--1041},
	file = {JSTOR Full Text PDF:/home/dg22309/Zotero/storage/YA43JKMI/Kitagawa - 1987 - Non-Gaussian State-Space Modeling of Nonstationary.pdf:application/pdf},
}

@article{athayde_forecasting_2022,
	title = {Forecasting {Covid}-19 in the {United} {Kingdom}: {A} dynamic {SIRD} model},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Forecasting {Covid}-19 in the {United} {Kingdom}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365164/},
	doi = {10.1371/journal.pone.0271577},
	abstract = {Making use of a state space framework, we present a stochastic generalization of the SIRD model, where the mortality, infection, and underreporting rates change over time. A new format to the errors in the Susceptible-Infected-Recovered-Dead compartments is also presented, that permits reinfection. The estimated trajectories and (out-of-sample) forecasts of all these variables are presented with their confidence intervals. The model only uses as inputs the number of reported cases and deaths, and was applied for the UK from April, 2020 to Sep, 2021 (daily data). The estimated infection rate has shown a trajectory in waves very compatible with the emergence of new variants and adopted social measures. The estimated mortality rate has shown a significant descendant behaviour in 2021, which we attribute to the vaccination program, and the estimated underreporting rate has been considerably volatile, with a downward tendency, implying that, on average, more people are testing than in the beginning of the pandemic. The evolution of the proportions of the population divided into susceptible, infected, recovered and dead groups are also shown with their confidence intervals and forecast, along with an estimation of the amount of reinfection that, according to our model, has become quite significant in 2021. Finally, the estimated trajectory of the effective reproduction rate has proven to be very compatible with the real number of cases and deaths. Its forecasts with confident intervals are also presented.},
	number = {8},
	urldate = {2023-05-31},
	journal = {PLoS ONE},
	author = {Athayde, Gustavo M. and Alencar, Airlane P.},
	month = aug,
	year = {2022},
	pmid = {35947603},
	pmcid = {PMC9365164},
	pages = {e0271577},
	file = {PubMed Central Full Text PDF:/home/dg22309/Zotero/storage/SSFPP68V/Athayde and Alencar - 2022 - Forecasting Covid-19 in the United Kingdom A dyna.pdf:application/pdf},
}

@article{doucett_rao-blackwellised_2000,
	title = {Rao-{Blackwellised} {Particle} {Filtering} for {Dynamic} {Bayesian} {Networks}},
	abstract = {Particle filters (PFs) are powerful sampling­ based inference/learning algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a principled way, any type of probabil­ ity distribution, nonlinearity and non-stationarity. They have appeared in several fields under such names as "condensation", "sequential Monte Carlo" and "survival of the fittest". In this pa­ per, we show how we can exploit the structure of the DBN to increase the efficiency of parti­ cle filtering, using a technique known as Rao­ Blackwellisation. Essentially, this samples some of the variables, and marginalizes out the rest exactly, using the Kalman filter, HMM filter, junction tree algorithm, or any other finite di­ mensional optimal filter. We show that Rao­ Blackwellised particle filters (RBPFs) lead to more accurate estimates than standard PFs. We demonstrate RBPFs on two problems, namely non-stationary online regression with radial ba­ sis function networks and robot localization and map building. We also discuss other potential ap­ plication areas and provide references to some fi­ nite dimensional optimal filters.},
	language = {en},
	author = {Doucett, Arnaud and de Freitast, Nando and Murphyt, Kevin and Russent, Stuart},
	year = {2000},
	file = {Doucett et al. - 2000 - Rao-Blackwellised Particle Filtering for Dynamic B.pdf:/home/dg22309/Zotero/storage/86PWGTZ6/Doucett et al. - 2000 - Rao-Blackwellised Particle Filtering for Dynamic B.pdf:application/pdf},
}

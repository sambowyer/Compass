---
title: "MCMC Part 2 -Metropolis-within Gibbs Algorithm"
author: "Sam Bowyer"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
  - \DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
  - \DeclareMathOperator*{\argmin}{argmin}
---

```{r setup, include=FALSE}
set.seed(1)
knitr::opts_chunk$set(echo = TRUE)
```

For this task we will again be using the Pima Indians Diabetes dataset from the `R` package `mlbench`. This contains $n=768$ input-output pairs $\{(y_i^0, x_i^0)\}_{i=1}^n$ with $y_i^0 \in \{0,1\}$ indicating whether patient $i$ has diabetes and $x_i^0 \in \mathbb{R}^p$ representing $p=8$ diagnostic measurements.
```{r}
library(mlbench)
data(PimaIndiansDiabetes)
head(PimaIndiansDiabetes)
dim(PimaIndiansDiabetes)
```

As before, we split up the data into inputs and outputs, and we will convert the $y_i^0$ types into zeroes and ones.
```{r}
p = ncol(PimaIndiansDiabetes)-1
X = PimaIndiansDiabetes[,1:p]
Y = as.numeric(PimaIndiansDiabetes[,p+1] == "pos")

head(X); head(Y)
dim(X); length(Y)
```

We consider the logistic regression model
\[Pr_{\alpha, \beta}^{(i)}(Y_i^0 = 1) = \frac{1}{1 + e^{-\alpha -\beta^T x_i^0}}, \;\; i = 1,...,n.\]

We put standard Gaussian priors on our parameters $\alpha$ and $\beta$, so $(\alpha, \beta) \sim \mathcal{N}_{p+1}(\mathbf{0}, \mathbf{I}_{p+1})$ and have a likelihood function
\[L_n(\alpha, \beta) = \prod_{i=1}^n Pr_{\alpha, \beta}^{(i)}(Y_i^0 = y_i^0).\]

From this, we arrive at the posterior distribution over $(\alpha, \beta)$ given the data:
\[\pi(\alpha, \beta | y^0) \propto L_n(\alpha, \beta) \pi(\alpha, \beta).\]

Below, we implement functions to calculate the log prior, likelihood and (unnormalised) posterior, as well as obtaining initial values for $\alpha$ and $\beta$ (which we represent together as a vector in $\mathbb{R}^{p+1}$). 
```{r}
log_prior <- function(params){
  return(sum(dnorm(params, log=TRUE)))
}

log_likelihood <- function(params){
  logits = params[1] + params[-1] %*% t(X)
  return(sum(Y*logits - log(1+exp(logits))))
}

log_posterior <- function(params){
  return(log_prior(params) + log_likelihood(params))
}

# initial params
params0 = rnorm(p+1)
params0
```

<!-- In the previous portfolio, for our proposal distribution $Q$ we used $Q(z, dz') = \mathcal{N}_{p+1}(z, c \Sigma_n)$ with a tuning parameter $c > 0$, where  -->
<!-- \[\begin{aligned} -->
<!-- \Sigma_n &= -(\mathbf{H}_n(\mu_n))^{-1} \\ -->
<!-- \mu_n &= \argmax_{(\alpha, \beta) \in \mathbb{R}^{p+1}} \log \pi(\alpha, \beta | y^0) \\ -->
<!-- \mathbf{H}_n(\theta) &= \left(\frac{\partial^2}{\partial\theta_l \partial \theta_l} \log \pi(\theta | y^0)\right)_{j,l=1}^{p+1}, \;\; \forall \theta \in \mathbb{R}^{p+1}. -->
<!-- \end{aligned}\] -->

<!-- We approximate $\mu_n$ using `optim` and calculate the Hessian $\mathbf{H}_n(\mu_n))$ using the `hessian` function from the package `pracma`. -->
<!-- ```{r} -->
<!-- mu_n = optim(par = params0, function(x) -log_likelihood(x))$par -->

<!-- library(pracma) -->
<!-- Sigma_n = -solve(hessian(f=log_posterior, x=mu_n)) -->
<!-- ``` -->

<!-- Here $Q(z, dz') = \mathcal{N}_{p+1} (\mu_n, \Sigma_n)$ is a Gaussian approximation of $\pi(\alpha, \beta | y^0)$, so in order to create our proposals $\{Q_j\}_{j=1}^{p+1}$ for each parameter in $\theta := (\alpha, \beta) \in \mathbb{R}^{p+1}$, for \textit{this} portfolio, we will use a conditional Gaussian distribution, where for each parameter $\theta_j \in \mathbb{R}$, $j \in \{1,...,p+1\}$, we condition on all other parameters $\theta_{j'}$, $j' \neq j$. -->

<!-- We can do this by first letting $\Sigma_{(-j, -j)}$ be the submatrix of $\Sigma_n$ with the $j$th row and column removed, $\Sigma_{(-j, j)} \in \mathbb{R}^p$ be the $j$th column of $\Sigma_n$ with the $j$th element removed, $\Sigma_{(j, -j)} = (\Sigma_{(-j,j)})^T$ and $\Sigma_{(j,j)}$ be the $(j,j)$th element of $\Sigma_n$. -->
<!-- Additionally, we define $\theta_{-j} \in \mathbb{R}^p$ to be the vector $\theta$ with its $j$th element, $\theta_j$, removed. -->
<!-- With this, the desired conditional distribution for a new sample $z_j$ in place of the old $\theta_j$, is given by -->
<!-- \[Q(z_j, dz'_j | \theta)= \mathcal{N}_p(z_j, c \hat{\Sigma}_{(j,j)}) \] -->
<!-- where -->
<!-- \[\begin{aligned} -->
<!-- \hat{\Sigma}^{(j)} &= \Sigma_{(j,j)} - \Sigma_{(j,-j)} (\Sigma_{(-j,-j)})^{-1} \Sigma_n(-j,j)} -->
<!-- \end{aligned}\] -->

<!-- <!-- NOT $\hat{z}_{j} &= z_j + \Sigma_{(j,-j)} (\Sigma_{(-j,-j)})^{-1}(\theta_{-j} - \mu_n^{(-j)})$ --> -->
<!-- ```{r} -->
<!-- library(mvtnorm) -->
<!-- proposal <- function(c, j){ -->
<!--   # calculate conditional mean and covariance -->
<!--   Sigma_hat = Sigma_n[j,j] - Sigma_n[j,-j] %*% solve(Sigma_n[-j,-j]) %*% Sigma_n[-j,j] -->

<!--   Q <- list() -->
<!--   Q$sample <- function(x){ -->
<!--     rnorm(1, mean = x, sd = c*Sigma_hat) -->
<!--   } -->
<!--   Q$density <- function(x,y){ -->
<!--     dnorm(y, mean = x, sd = c*Sigma_hat) -->
<!--   } -->
<!--   return(Q) -->
<!-- } -->
<!-- ``` -->

<!-- We can generate our full list of proposals into a list as follows. -->
<!-- ```{r} -->
<!-- generateProposalsList <- function(c){ -->
<!--   conditionals = list() -->
<!--   for (j in 1:(p+1)){ -->
<!--     conditionals[[j]] = proposal(c,j) -->
<!--   } -->
<!--   return(conditionals) -->
<!-- } -->
<!-- ``` -->

We use standard normal proposals for each of the elements of $(\alpha, \beta()$  (though these need not be identical) and use the Metropolis-within-Gibbs algorithm presented below.
```{r}
proposal <- function(c){
  Q <- list()
  Q$sample <- function(x){
    rnorm(1, mean = x)#, sigma = 1)
  }
  Q$density <- function(x,y){
    dnorm(y, mean = x)#, sigma = 1)
  }
  return(Q)
}

runMwG <- function(f, proposal, x0, n, c){
  xs = matrix(rep(0, n*(p+1)), nrow=n)
  x = x0
  acceptanceCount = 0
  
  for(i in 1:n){
    for (j in 1:(p+1)){
      # Q = proposalList[[j]]
      Q = proposal(c)
      
      q = Q$density
      z = Q$sample(x)
      
      new_x = x
      new_x[j] = z
      # print("start")
      # print(f(new_x))
      # print(z,x[j])
      # print(f(x))
      # print(q(x[j],z))
      
      acceptProb = min(1, exp(log(f(new_x)) + log(q(z,x[j])) - log(f(x)) - log(q(x[j],z))))
      if(runif(1)<acceptProb){
        x = new_x
        acceptanceCount = acceptanceCount + 1
      }
    }
    xs[i,] = x
  }
  return(list(samples = xs, acceptanceRate = acceptanceCount/(n*(p+1))))
}
```

First we use $c=1$ and obtain very good results after generating 100000 samples.
```{r, warning=FALSE, cache=TRUE}
n = 100000
c = 1
# Qs = generateProposalsList(c)
MH = runMwG(function(x) exp(log_posterior(x)), proposal, rnorm(p+1), n, 1)
```

In particular, we observe an acceptance rate of 0.19538, somewhat close to the theoretical optimal rate of 0.234.
```{r}
MH$acceptanceRate
```

Furthermore, looking at the trace plots for the generated samples we observe healthy looking convergence after a small amount of mixing time.
```{r}
var_names = c("alpha")
for (i in 1:9){
  var_names = c(var_names, paste("beta_", i))
}

par(mfrow=c(3,3))
for (i in 1:9){
  plot(1:n, MH$samples[,i], type="l", xlab="Sample No.", ylab=paste(var_names[i]))
}
```

We also see autocorrelation plots that diminish fairly quickly towards 0.
```{r}
par(mfrow=c(3,3))
for (i in 1:9){
  acf(MH$samples[,i], lag.max=250, main="")
  title(var_names[i])
}
```

This culminates in acceptable-looking plots for estimated marginal posterior distributions for each of the model's nine parameters---each of which are unimodal and vaguely Gaussian.
```{r}
par(mfrow=c(3,3))
for (i in 1:9){
  plot(density(MH$samples[,i]), xlab="Sample No.", ylab=paste(var_names[i]), main="Marginal Density")
}
```

For comparison against a badly-tuned chain, consider the results when we use $c=0.001$ (again with 100000 generated samples).

```{r, warning=FALSE, cache=TRUE}
c = 0.001
MH = runMH(function(x) exp(log_posterior(x)), proposal(c), mu_n, n)
```

This gives an acceptance rate of 0.96417, far above the optimal value which suggests that the proposal is failing to capture the behaviour of the posterior.
```{r}
MH$acceptanceRate
```

Using these samples we see much worse convergence than before in the trace plots.
```{r}
par(mfrow=c(3,3))
for (i in 1:9){
  plot(1:n, MH$samples[,i], type="l", xlab="Sample No.", ylab=paste(var_names[i]))
}
```

Similarly, the autocorrelation stays relatively large even as we increase the lag, further suggesting that the proposal isn't exploring the posterior space successfully.
```{r}
par(mfrow=c(3,3))
for (i in 1:9){
  acf(MH$samples[,i], lag.max=250, main="")
  title(var_names[i])
}
```

And finally when we plot the estimated marginal posterior distributions we see plots that looks a lot less Gaussian and unimodal.
```{r}
par(mfrow=c(3,3))
for (i in 1:9){
  plot(density(MH$samples[,i]), xlab="Sample No.", ylab=paste(var_names[i]), main="Marginal Density")
}
```
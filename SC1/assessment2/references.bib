
@inproceedings{legland_recursive_1997,
	address = {San Diego, CA, USA},
	title = {Recursive estimation in hidden {Markov} models},
	volume = {4},
	isbn = {978-0-7803-4187-6},
	url = {http://ieeexplore.ieee.org/document/652384/},
	doi = {10.1109/CDC.1997.652384},
	abstract = {We consider a hidden Markov model (HMM) with multidimensional observations, and where the coefficients (transition probability matrix, and observation conditional densities) depend on some unknown parameter. We study the asymptotic behaviour of two recursive estimators, the recursive maximum likelihood estimator (RMLE), and the recursive conditional least squares estimator (RCLSE), as the number of observations increases to infinity. Firstly, we exhibit the contrast functions associated with the two non-recursive estimators, and we prove that the recursive estimators converge a.s. to the set of stationary points of the corresponding contrast function. Secondly, we prove that the two recursive estimators are asymptotically normal.},
	language = {en},
	urldate = {2022-11-24},
	booktitle = {Proceedings of the 36th {IEEE} {Conference} on {Decision} and {Control}},
	publisher = {IEEE},
	author = {LeGland, F. and Mevel, L.},
	year = {1997},
	pages = {3468--3473},
	file = {LeGland and Mevel - 1997 - Recursive estimation in hidden Markov models.pdf:/home/dg22309/Zotero/storage/QBBEVNVR/LeGland and Mevel - 1997 - Recursive estimation in hidden Markov models.pdf:application/pdf},
}

@article{rydkn_recursive_nodate,
	title = {On recursive estimation for hidden {Markov} models},
	abstract = {Hidden Markov models (HMMs) have during the last decade become a widespread tool for modelling sequences of dependent random variables. In this paper we consider a recursive estimator for HMMs based on the m-dimensional distribution of the process and show that this estimator converges to the set of stationary points of the corresponding Kullback-Leibler information. We also investigate averaging in this recursive scheme and show that conditional on convergence to the true parameter, and provided m is chosen large enough, the averaged estimator is close to optimal.},
	language = {en},
	author = {Rydkn, Tobias},
	pages = {18},
	file = {Rydkn - On recursive estimation for hidden Markov models.pdf:/home/dg22309/Zotero/storage/XLT75TGL/Rydkn - On recursive estimation for hidden Markov models.pdf:application/pdf},
}

@article{slingsby_recursive_1992,
	title = {Recursive {Parameter} {Estimation} for {Arbitrary} {Hidden} {Markov} {Models}},
	volume = {25},
	issn = {14746670},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S147466701750703X},
	doi = {10.1016/S1474-6670(17)50703-X},
	abstract = {This paper describes on-line re-estimation formulae for arbitrary hidden Markov models, extending previous work that proposed a recursive formula for re-estimation of a diagonally dominant state transition probability matrix. Simulation experiments show that for some applications the formula described here should be used instead in order to ensure algorithm convergence. Also proposed is a re-estimation formula for online update of non-parametric observation probabilities. Simulation experiments show that these estimates are in general asymptotically biased.},
	language = {en},
	number = {14},
	urldate = {2022-11-24},
	journal = {IFAC Proceedings Volumes},
	author = {Slingsby, P.L.},
	month = jul,
	year = {1992},
	pages = {1--4},
	file = {Slingsby - 1992 - Recursive Parameter Estimation for Arbitrary Hidde.pdf:/home/dg22309/Zotero/storage/R5FHQRY6/Slingsby - 1992 - Recursive Parameter Estimation for Arbitrary Hidde.pdf:application/pdf},
}

@article{frazzoli_16410_nodate,
	title = {16.410 {Lecture} 21: {Intro} to {Hidden} {Markov} {Models} the {Baum}-{Welch} algorithm},
	language = {en},
	author = {Frazzoli, Emilio},
	pages = {24},
	file = {Frazzoli - 16.410 Lecture 21 Intro to Hidden Markov Models t.pdf:/home/dg22309/Zotero/storage/ZYQ9WIS4/Frazzoli - 16.410 Lecture 21 Intro to Hidden Markov Models t.pdf:application/pdf},
}

@article{yoon_hidden_2009,
	title = {Hidden {Markov} {Models} and their {Applications} in {Biological} {Sequence} {Analysis}},
	volume = {10},
	issn = {1389-2029},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/},
	doi = {10.2174/138920209789177575},
	abstract = {Hidden Markov models (HMMs) have been extensively used in biological sequence analysis. In this paper, we give a tutorial review of HMMs and their applications in a variety of problems in molecular biology. We especially focus on three types of HMMs: the profile-HMMs, pair-HMMs, and context-sensitive HMMs. We show how these HMMs can be used to solve various sequence analysis problems, such as pairwise and multiple sequence alignments, gene annotation, classification, similarity search, and many others.},
	number = {6},
	urldate = {2022-11-27},
	journal = {Current Genomics},
	author = {Yoon, Byung-Jun},
	month = sep,
	year = {2009},
	pmid = {20190955},
	pmcid = {PMC2766791},
	pages = {402--415},
}

@article{rabiner_tutorial_1989,
	title = {A tutorial on hidden {Markov} models and selected applications in speech recognition},
	volume = {77},
	issn = {1558-2256},
	doi = {10.1109/5.18626},
	abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{\textless}{\textgreater}},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Rabiner, L.R.},
	month = feb,
	year = {1989},
	keywords = {Hidden Markov models, Speech recognition, Tutorial},
	pages = {257--286},
}

@article{wong_dna_2013,
	title = {{DNA} motif elucidation using belief propagation},
	volume = {41},
	issn = {0305-1048},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3763557/},
	doi = {10.1093/nar/gkt574},
	abstract = {Protein-binding microarray (PBM) is a high-throughout platform that can measure the DNA-binding preference of a protein in a comprehensive and unbiased manner. A typical PBM experiment can measure binding signal intensities of a protein to all the possible DNA k-mers (k = 8 ∼10); such comprehensive binding affinity data usually need to be reduced and represented as motif models before they can be further analyzed and applied. Since proteins can often bind to DNA in multiple modes, one of the major challenges is to decompose the comprehensive affinity data into multimodal motif representations. Here, we describe a new algorithm that uses Hidden Markov Models (HMMs) and can derive precise and multimodal motifs using belief propagations. We describe an HMM-based approach using belief propagations (kmerHMM), which accepts and preprocesses PBM probe raw data into median-binding intensities of individual k-mers. The k-mers are ranked and aligned for training an HMM as the underlying motif representation. Multiple motifs are then extracted from the HMM using belief propagations. Comparisons of kmerHMM with other leading methods on several data sets demonstrated its effectiveness and uniqueness. Especially, it achieved the best performance on more than half of the data sets. In addition, the multiple binding modes derived by kmerHMM are biologically meaningful and will be useful in interpreting other genome-wide data such as those generated from ChIP-seq. The executables and source codes are available at the authors’ websites: e.g. http://www.cs.toronto.edu/∼wkc/kmerHMM.},
	number = {16},
	urldate = {2022-11-27},
	journal = {Nucleic Acids Research},
	author = {Wong, Ka-Chun and Chan, Tak-Ming and Peng, Chengbin and Li, Yue and Zhang, Zhaolei},
	month = sep,
	year = {2013},
	pmid = {23814189},
	pmcid = {PMC3763557},
	pages = {e153},
}
